---
title: Matrix Decomposition Factor Analysis
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
source("mdfaOptim.R")
matrixPrint <- function(x,
                        digits = 6,
                        width = 8,
                        format = "f",
                        flag = "+") {
  print(noquote(
    formatC(
      x,
      digits = digits,
      width = width,
      format = format,
      flag = flag
    )
  ))
}
```

\sectionbreak

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All qmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://www.github.com/deleeuw/mdfa> 

\sectionbreak

# Introduction

Suppose $X$ is an $n\times m$ "tall" data matrix ($n\geq m$). 
We say that $X=YT'$, with $Y$
an $n\times p$ matrix and $T$ an $m\times p$ matrix, is a 
*decomposition of order $p$* of $X$. Decompositions of various kinds play a key role in theoretical and numerical linear algebra (@stewart_98). 

In this paper
we are interested in the *factor analytic decomposition of order $p$*,
which is characterized by $m\leq p\leq n$ and by the requirement $Y'Y=I$. In addition there may be constraints on $T$, which we write in the general form $T\in\mathcal{T}$, with $\mathcal{T}$
a subset of $\mathbb{R}^{m\times p}$, the space of all $m\times p$ matrices.
Note that $Y$ is a "tall" matrix ($n\geq p$), while $T$ is "wide" ($m\leq p$).

Factor analysis techniques aim to find a solution of the system
\begin{subequations}
\begin{align}
X&=YT',\label{eq-fa1}\\
Y'Y&=I,\label{eq-fa2}\\
T&\in\mathcal{T}.\label{eq-fa3}
\end{align}
\end{subequations}
If no exact solution exists an approximate solution must be found.
Mathematically this introduces the problem to find the conditions
under which equations \eqref{eq-fa1}-\eqref{eq-fa3} can be solved for $Y$ and $T$, 
and to describe the set of solutions if the system is solvable.
Computationally the problem is to define what is meant by 
"approximately" and to find a technique that produces an
approximate solution. Typically this is done by defining a
non-negative loss function that measures departure from perfect fit and
an algorithm for minimizing it. 

In *Common Factor Analysis (CFA)*, which is the most important special case of 
factor analytic decomposition, the set $\mathcal{T}$ is the set of
partitioned matrices $\begin{bmatrix}A&\mid&D\end{bmatrix}$. The
matrix of *common factor loadings* $A$ is $m\times q$, with\footnote{The symbol $:=$ is used for definitions.} $q:=p-m$, and the matrix of *unique factor loadings* $D$ is diagonal of order $m$. In CFA parlance the diagonal elements of $D^2$ are called *uniquenesses*. In addition there can be constraints on $A$, which we write as $A\in\mathcal{A}$. If $A$ is unrestricted the CFA is *exploratory*, otherwise it is *confirmatory*. There is a corresponding partition of $Y$ as $\begin{bmatrix}F&\mid&U\end{bmatrix}$,
with $F$ the $n\times q$ matrix of *common factor scores* and $U$ the $n\times m$ matrix
of *unique factor scores*. 

For CFA the system \eqref{eq-fa1}-\eqref{eq-fa3} thus becomes
\begin{subequations}
\begin{align}
X&=FA'+UD,\label{eq-cfa1}\\
F'F&=I,\label{eq-cfa2}\\
U'U&=I,\label{eq-cfa3}\\
F'U&=0,\label{eq-cfa4}\\
D&=\text{diag(D)},\label{eq-cfa5}\\
A&\in\mathcal{A}.\label{eq-cfa6}
\end{align}
\end{subequations}

Although CFA will always be in the back of our mind, we will develop equations and algorithms
for the general case $X=YT$ with $T\in\mathcal{T}$.

\sectionbreak

# The Fundamental Theorem

In this section we look at finding exact solutions of the "full" system of equations
\eqref{eq-fa1}-\eqref{eq-fa3}. This shows under what conditions we can expect
perfect fit and a zero value for the minimum of our loss function. As we will see, 
the basic solvability result, which we call the "Fundamental Theorem of Factor
Analysis" following @kestelman_52, also has computational consequences.

We start by considering the "reduced" system, using $C:=X'X$,
\begin{subequations}
\begin{align}
C&=TT',\label{eq-pfa1}\\
T&\in\mathcal{T}.\label{eq-pfa2}
\end{align}
\end{subequations}
Solvability of the reduced system is a necessary condition for solvability
of the full system \eqref{eq-fa1}-\eqref{eq-fa3}.

Most factor analysis procedures are two-step methods. In the first step they find
an approximate solution $T$ to the reduced system, and then use this 
$T$ to find an approximate solution $Y$ to the full system. The two steps may 
actually use two different loss functions, the first one to assess 
the fit of $C=TT'$ and the second one the fit of $X=YT'$. This practice is motivated, and to some extent justified, by the following theorem, first
proved by @garnett_19.

::: {#thm-fundamental}

## Fundamental Theorem of Factor Analysis

The *full* system \eqref{eq-fa1}-\eqref{eq-fa3} is solvable if and only if the *reduced* system \eqref{eq-pfa1}-\eqref{eq-pfa2} is solvable.
:::
::: {.proof}
Necessity is trivial. For sufficiency we have to prove that if $T$ is any solution of the
reduced system then there is a $Y$ such that $(T,Y)$ satisfies the full system.
Our proof uses the "fat" singular value decomposition (SVD) of the $n\times m$ matrix $X$, assumed to be of rank $r$, which is
\begin{equation}
X_{n\times m}=
\begin{bmatrix}
K_{n\times r}^1&K^0_{n\times(n-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(m-r)}\\
0_{(n-r)\times r}&0_{(n-r)\times(m-r)}
\end{bmatrix}
\begin{bmatrix}
L_{m\times r}^1&L_{m\times(m-r)}^0
\end{bmatrix}'.\label{eq-fatx}
\end{equation}
The singular values in $\Lambda_{r\times r}$ are positive and decrease along the diagonal. Subscripts are used to indicate the dimension of the matrices.

From $C=TT'$ we know that a "fat" SVD of $T$ is
\begin{equation}
T_{m\times p}=
\begin{bmatrix}
L^1_{m\times r}&L^0_{m\times(m-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(p-r)}\\
0_{(m-r)\times r}&0_{(m-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',\label{eq-fatt}
\end{equation}
for some square orthonormal $M$. 

Write $Y$ as
\begin{equation}
Y_{n\times p}=
\begin{bmatrix}
K^1_{n\times r}&K^0_{n\times(n-r)}
\end{bmatrix} 
\begin{bmatrix}
S^{11}_{r\times r}&S^{10}_{r\times (p-r)}\\S^{01}_{(n-r)\times r}&S^{11}_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',\label{eq-writey}
\end{equation}
where the partitioned matrix $S$ in the middle must satisfy $S'S=I$.

Now $X=YT'$ becomes
\begin{multline}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(m-r)}\\
0_{(n-r)\times r}&0_{(n-r)\times(m-r)}
\end{bmatrix}=
\\
\begin{bmatrix}
S^{11}_{r\times r}&S^{10}_{r\times (p-r)}\\S^{01}_{(n-r)\times r}&S^{11}_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times (m-r)}\\
0_{(p-r)\times r}&0_{(p-r)\times(m-r)}
\end{bmatrix},\label{eq-prodyt}
\end{multline}
which implies  
\begin{equation}
S=\begin{bmatrix}
I_{r\times r}&0_{r\times(p-r)}\\
0_{(n-r)\times r}&V_{(n-r)\times(p-r)}
\end{bmatrix}\label{eq-solves}
\end{equation}
where $V$ satisfies $V'V=I$, but is otherwise arbitrary.

From \eqref{eq-writey} and \eqref{eq-solves} we see that
\begin{equation}
Y=K_1M_1'+K_0VM_0'\label{eq-yperf}
\end{equation}
provides us with the required solution of the full system.
:::

If the non-zero singular values of $X$, and consequently of $T$,  are all different then $K_1$ and 
$M_1$ are uniquely determined. Matrices $K_0$ and 
$M_0$ consist of orthonormal bases for the null-spaces of $X$ and $T$,
which are only determined up to rotations. We can select any one of these bases
and absorb the rotations in the arbitrary matrix $V$.
The fundamental theorem implies that for any solution $T\in\mathcal{T}$ of $C=TT'$ there is a linear subspace of solutions $Y$ of $X=YT'$. Even if $T$ is identified, $Y$ is not. This is the *factor indeterminacy problem*, which has haunted the field for more than 100 years.
(@steiger_schoenemann_78).

\sectionbreak

# Least Squares Factor Analysis

## MINRES

There are several factor analytic decomposition techniques that use least squares loss functions\footnote{We use SSQ() forthe unweighted sum of squares of a matrix or vector.}. The oldest and most obvious one is
\begin{equation}
\sigma(T)=\text{SSQ}(C-TT'),\label{eq-minres}
\end{equation}
where $\text{SSQ}()$ stands for ther unweighted sum of squares.
For CFA \eqref{eq-minres} becomes
\begin{equation}
\sigma(T)=\text{SSQ}(C-AA'-D^2).\label{eq-minrescfa}
\end{equation}
An alternating least squares technique to minimize \eqref{eq-minrescfa} was first proposed by @thomson_34. It alternates minimizing the loss function \eqref{eq-minrescfa} over $D$ for the current $A$ and minimizing over $A$ for the current $D$. Of course this only finds an approximate solution to the reduced system, and it leaves open the question on how to compute the factor scores $Y$. Alternative algorithms for minimizing \eqref{eq-minrescfa}
are MINRES of @harman_jones_66 and the Newton-Raphson technique of @derflinger_69 and @joreskog_vanthillo_71.

Also for CFA @young_40 and @whittle_52 propose minimizing the weighted least 
squares loss function
\begin{equation}
\sigma(A, D, F)=\text{tr}\ (X-FA')D^{-2}(X-FA')',\label{eq-young-whittle}
\end{equation}
which has the disadvantage that it assumes $D$ is known, or at least that there is a reasonable prior estimate of $D$. Minimizing \eqref{eq-young-whittle} has the advantage that the problem becomes a form of principal component analysis (PCA), in which the solution can be computed with a single SVD.

It is tempting to use block relaxation heto minimize \eqref{eq-young-whittle}, alternating finding the optimum
$F$ and $A$ for fixed $D$, and then repeating this iteratively with the new $D$ equal to
\begin{equation}
D^2=n^{-1}\text{diag}\ (X-FA')'(X-FA').\label{eq-yw}
\end{equation}
This in fact corresponds to alternating minimization of the loss function
\begin{equation}
\sigma(A,F,D)=n\log\text{det}(D^2)+\text{tr}\ (X-FA')D^{-2}(X-FA')'.\label{eq-lawley41}
\end{equation}
But \eqref{eq-lawley41} is the negative likelihood loss function for the fixed factor model from @lawley_41, and we know this loss function is unbounded below, does not have a minimum, and converges to a perfect but trivial solution which has $p$ of the uniquenesses in $D^2$ equal to zero (@anderson_rubin_56).

## MDFA

A more direct way of fitting the full CFA model was discussed (independently and around the same time) in the dissertation of @socan_03 and in the conference chapter of @deleeuw_C_04a (presented at the conference in 2002). Recently this technique has become known as Matrix Decomposition Factor analysis (MDFA). Socan attributes MDFA (which they call "Direct Factor Analysis") to a 2001 personal communication and some unpublished notes of his advisor Henk Kiers.

In MDFA the least squares loss function is
\begin{equation}
\sigma(Y,A):=\text{SSQ}(X-YT'),\label{eq-lossdef}
\end{equation}
which must be minimized over $Y'Y=I$ and $T\in\mathcal{T}$.
MDFA was not immediately accepted as an alternative factor analysis technique. It made its first journal appearance in a series of papers by Unkel and Trendafilov, based largely on Unkel's dissertation (@unkel_09). Over the years they contributed a "robust"
version of MDFA (@unkel_trendafilov_10) and a version for a "wide" data matrix $X$ (@trendafilov_unkel_11). There is a nice review of these contributions in @unkel_trendafilov_10, with an update in @trendafilov_unkel_krzanowski_13.

There have been additional important contributions to MDFA in @adachi_12, @adachi_trendafilov_18,
@stegeman_16, @terada_25, @yamashita_25. We will discuss these recent contributions in various places in this paper. When googling MDFA, keep in mind that Adachi initially used "Data-Fitting Factor Analysis", while Stegeman used the "Data Factor Model". 


\sectionbreak

# MDFA Algorithms

## Alternating Least Squares

The Alternating Least Squares (ALS) algorithm proposed for MDFA by both Kiers (in @socan_03) and @deleeuw_C_04a alternates finding the optimum $Y$ for given $T$ and the optimum $T$ for given $Y$.

### Optimum Loadings for Given Scores

Finding the optimum $T$ for given $Y$ is  straightforward.
We complete the square, as in
\begin{equation}
\sigma(Y,T)=
\text{tr}\ C+\text{tr} (T-X'Y)'(T-X'Y)-\text{tr}\ Y'CY.
\end{equation}
Thus the optimum $T$ for given $Y$ is obtained by projecting 
$X'Y$ on the set of matrices $\mathcal{T}$. 

For exploratory CFA this gives 
$$
T=\begin{bmatrix}A&D\end{bmatrix}=\begin{bmatrix}X'F&\text{diag}(X'U)\end{bmatrix}.
$$ 
For confirmatory MDFA in which we require some loadings to be equal to each other and/or others to be equal to given constants finding $T$ is still a simple linear least squares problem. If there are inequality constraints we need some form of quadratic programming.

### Optimum Scores for given Loadings

Also
\begin{equation}
\sigma(T,Y)=\text{tr}\ C+\text{tr}\ T'T-2\ \text{tr}\ Y'XT.
\end{equation}
We have to maximize $\text{tr}\ Y'XT$ over all
$n\times p$ orthonormal $Y$.
The following theorem
was given earlier, using somewhat different terminology, in the Appendix of @deleeuw_C_04a. Remember that the *nuclear norm* (a.k.a. the 
*trace norm*) $\|Z\|_\star$ of a matrix $Z$ is the sum of its singular values. The nuclear norm is also equal to the sum of the square roots of the eigenvalues of $Z'Z$ and thus equal to the trace of $(Z'Z)^{1/2}$.

::: {#thm-procrustus}
$$
\max_{Y'Y=I}\text{tr}\ Y'XT=\|XT\|_\star
$$
:::

:::{.proof}
The stationary equations are $XT=YM$ with $Y'Y=I$ and $M$ a symmetric matrix of Lagrange multipliers of order $p$. It follows that $M^2=T'Y'YT$ and thus $M$ is the (unique) positive semi-definite
symmetric square root $\smash{(T'Y'YT)^{1/2}}$
(or its negative, for a minimum). At the maximum $\text{tr}\ Y'XT=\text{tr}\ M=\|XT\|_\star$.
:::

Note that @thm-procrustus does not say that the optimizing $Y$ is unique. We get more insight from an alternative and more constructive proof (also taken from @deleeuw_C_04a, Appendix). It uses the "fat" singular value decomposition of the $n\times p$ matrix $XT$ of rank $m$.

::: {#thm-construct}
The $n\times m$ orthonormal $Y$ maximizes $\text{tr}\ Y'XT$, with 
\begin{equation}
XT=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}\Lambda&0\\0&0\end{bmatrix}
\begin{bmatrix}L'&L_\perp'\end{bmatrix},\label{eq-xa}
\end{equation}
if and only if
\begin{equation}
Y=KL'+K_\perp^{\ }SL_\perp',\label{eq-yopt}
\end{equation}
for some $(n-r)\times(m-r)$ matrix $S$ with $S'S=I$.
:::

::: {.proof}
Partition the columnwise orthonormal $n\times m$ matrix 
$Y$ in the same way as $XT$.
\begin{equation}
Y=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}P&Q\\R&S\end{bmatrix}
\begin{bmatrix}L'&L_\perp'\end{bmatrix},\label{eq-odecomp}
\end{equation}
where the four-block partitioned matrix in the middle of \eqref{eq-odecomp} must be orthonormal again. Now $\text{tr}\ Y'XA=\text{tr}\ \Lambda P$ and the constraint on the $r\times r$ matrix $P$ is that $P'P\lesssim I$ in the Loewner\footnote{$A\lesssim B$ means $A-B$ positive semi-definite.} sense (because we must have
$P'P+R'R=I$). It follows that the diagonal elements of $P$ must be less than or equal to one, and thus the maximum of
$\text{tr}\ Y'XT$ is $\text{tr}\ \Lambda$, uniquely attained for $P=I$.
But $P=I$ implies that at the maximum $R=0$ and $Q=0$, and $S$ is any
$(n-r)\times(m-r)$ matrix with $S'S=I$. Thus the optimum $Y$ is
as given by \eqref{eq-yopt}.
:::

It is important to note that if $Y=KL'$ then $Y'Y=LL'$, which is of rank $m<p$ and thus not equal to
the identity. Also note that formula \eqref{eq-yopt} is identical to formula \eqref{eq-yperf} in the fundamental theorem. This is not surprising, since both compute the optimal $Y$ for given $T$. 

### CFA Variation

$$
\sigma(F,A,U,D)=SSQ(X-FA'-UD)
$$
$A=X'F$ and $D=\diag(X'U)$

Maximize $F'(X-UD)$, maximize $U'(X-FA')$


### Algorithm

Write $\Pi_Y()$ for the least squares projection of
a matrix on the columnwise orthonormal matrices of the same
dimension. Also write $\Pi_T()$ for least squares
projection on $\mathcal{T}$. Then ALS iteration $k$ is
\begin{subequations}
\begin{align}
Y^{(k)}&=\Pi_Y(XT^{(k)}),\label{eq-als1}\\
T^{(k+1)}&=\Pi_T(X'Y^{(k)}).\label{eq-als2}
\end{align}
\end{subequations}
It  does not matter how we choose $S$ in \eqref{eq-yopt}, because for all
choices of $S$ we have $\text{tr}\ Y'XA=\|XA\|_\star$, and thus all choices of $S$ give
the same loss  function value.

In most factor analytic applications we have $n>>m$. The computations 
in \eqref{eq-als1} and \eqref{eq-als2} use the matrices $X$ and $Y$, which each
have $n$ rows. Every iteration involves multiplications with these
large matrices. This will tend to be 
expensive computationally. It has been pointed out by @adachi_12
that we can rewrite the ALS algorithm so that it only 
involves matrices of order $m$. This alternative derivation of
the algorithm has the additional property that it can be
applied to $C$ without having to know $X$, which has obvious
advantages for secondary analysis.

::: {#thm-als}
Suppose $T^{(k)}$ has rank $m$. Then
\begin{equation}
T^{(k+1)}=\Pi_T\{CT^{(k)}(T^{(k)}CT^{(k)})^{-1/2}\}.\label{eq-alsshort}
\end{equation}
:::
::: {.proof}
We rewrite \eqref{eq-yopt} as
$$
Y=XT(T'CT)^{-1/2}+K_\perp^{\ } SL_\perp'
$$
where $(T'CT)^{-1/2}$ is the square root of the Moore-Penrose inverse of 
$T'CT$. Thus
$$
T^{(k+1)}=X'Y=CT^{(k)}(\{T^{(k)}\}'CT^{(k)})^{-\frac12}+X'K_\perp^{\ } SL_\perp'
$$
We have $K_\perp'XT=0$. If $T$ has rank $m$ then this implies
$X'K_\perp=0$
which leads to the result in the theorem.
:::

Algorithm \eqref{eq-alsshort} requires us to compute $(T'CT)^{-\frac12}$ in each iteration, which 
means finding the $m$ largest eigenvalues and their corresponding eigenvectors. The update is then
projected on the set $\mathcal{T}$ defined by the template. The ALS interpretation of the
algorithm shows directly that a decreasing sequence of loss function values is produced, which
proves convergence of $T$ under the usual identification and rank conditions.

\sectionbreak


## Gradient and Newton Methods


### Gradient

$$
\sigma_\star(T)=\text{tr}\ X'X+\text{tr}\ T'T-2\|XT\|_\star=\text{tr}\ X'X+\text{tr}\ T'T-2\sum_{\nu=1}^m\lambda_\nu^\frac12(T'CT)
$$

First and second derivatives of the trace norm with respect to $T$ were computed
in @deleeuw_E_25d. The definition of $\mathcal{T}$ that was used is
$$
T=T_0+\sum_{s=1}^q\theta_s T_s,
$$
which allows for patterns with some elements fixed and some elements equal to other elements.
$$
\mathcal{D}_s\sigma_\star=\text{tr}\ T'T_s-\text{tr}\ (T'CT)^{-1/2}T'CT_s
$$

The derivatives of the loss function ,,, with respect to the parameters $\theta_s$ are most easily expressed by first computing the derivatives of the eigenvalues of $T'CT$.
\begin{equation}
\mathcal{D}_s\lambda_\nu=x_\nu'Q_sx_\nu,\label{eq-mdfadl}
\end{equation}
where $Q_s:=T_s'CT+T'CT_s$. It follows that
\begin{equation}
\mathcal{D}_{st}\lambda_\nu=-2x_\nu'Q_s(A-\lambda_\nu I)^+Q_tx_\nu+2x_\nu'T_s'CT_tx_\nu.\label{eq-mdfaddl}
\end{equation}



With this formula for the gradient many general purpose optimization methods for minimizing projected loss function ... become available. The R function
... (code in the appendix) uses the BFGS method of the optim() function from the stats package
(@r_core_team_25). Our example is the correlation matrix of order 9 from 
@emmett_49, also used in @lawley_maxwell_71, pp. 42-46. We use three
common factors, and compare our results with those obtained using
multinormal maximum likelihood (@lawley_maxwell_71, table 4.2).

```{r}
matrixPrint(emmett, digits = 3, width = 5, flag = "")
```
```{r}
aeig <- eigen(emmett)
avec <- aeig$vectors[, 1:3]
aval <- diag(sqrt(aeig$values[1:3]))
acom <- avec %*% aval
a <- cbind(acom, diag(sqrt(1 - rowSums(acom^2))))
t <- cbind(matrix(1, 9, 3), diag(9))
a[1, 2] <- a[1, 3] <- a[2, 3] <- 0
t[1, 2] <- t[1, 3] <- t[2, 3] <- 0
h <- optProj(emmett, a, t)
print(h)
```
```{r}
a <- cbind(acom, diag(sqrt(1 - rowSums(acom^2))))
t <- cbind(matrix(1, 9, 3), diag(9))
h <- optProj(emmett, a, t)
print(h)
```

### Hessian


\sectionbreak

# Discussion

## Additional Constraints

From the CFA definition ...-... we see that $A=X'F$ and 
$D=X'U$. In an exploratory MDFA solution we do have
$A=X'F$ but $D=\text{diag}(X'U)$. As a consequence
unique factor $i$ in the solution can be correlated
with variables $j$, where $j\neq i$. This is contrary
the very idea of unique factors. If $X'U$ is not
diagonal then this indicates a lack of CFA fit. It may also be sufficiently counter-intuitive to prevent it from
happening.

In the thesis of Socan a variation of MDFA is discussed in which it is required, in addition to $Y'Y=I$, that $U'X$ is diagonal. In the version of MDFA in @stegeman_16, which they call the *Data Factor Model*,
it is even required that $U'X=D$. In the case of perfect fit the constraint $U'X=D$ is automatically
satisfied, but it will not be so for the least squares fit. These additional constraints
on MDFA complicate the ALS algorithm and take us in the direction of alternative methods for 
computing factor scores. We do not impose those constraints in this paper.

## Correlated Errors

## Heywood Cases

The CFA equations $C=AA'+D^2$ imply that $0\lesssim D^2\lesssim C$ and $0\lesssim AA'\lesssim C$. This in turn implies that $D^2\lesssim\text{diag}(C)$.
In many forms of CFA it is possible that the estimated
uniqueness \\

::: {#thm-bounds}
In the MDFA solution $AA'\lesssim C$ and 
$0\lesssim D^2\lesssim\text{diag}(C)$.
:::
::: {.proof}
:::

Note that we have not shown that $D^2\lesssim C$

## Scale Free

An MDFA procedure $X\rightarrow(Y,T)$ maps data matrices
$X$ to a pair of scores $Y$ and loadings $T$. The
procedure is scale-free if for all diagonal $\Phi$
we have $X\Phi\rightarrow(Y,T\Phi)$. The MDFA 
procedure we have discussed so far is not scale-free, but 
it can be made scale-free in various ways.

The first way is traditional. We normalize the data
in $X$ to unit length, so that $C=X'X$ becomes a
correlation matrix. This can be formalized as 
minimizing
$$
\sigma_W(T,Y):=\text{tr}\ (X-YT')W(X'-TY')
$$
with $W:=\text{diag}^{-1}(C)$. @adachi_12
proposes instead to use $\smash{W=C^{-1}}$.
But no matter how we choose the positive definite
$W$ the computational consequences are
easy to handle.

We have
$$
\sigma_W(T,Y)=\text{tr}\ XWX'+\text{tr}\ (T-X'Y)'W(T-X'Y)-\text{tr}\ Y'XWX'Y
$$
and consequently the optimal $T$ for given $Y$ is the
projection in the metric $W$ of $X'Y$ on $\mathcal{T}$.
This may or may not be more complicated than unweighted
projection, depending on the definition of $\mathcal{T}$.

Also
$$
\sigma_W(T,Y)=\text{tr}\ XWX'+\text{tr}\ T'WT-2\ \text{tr}\ Y'XWT,
$$
which means the optimal $Y$ for given $T$ is the 
(unweighted) projection of $XWT$ on the orthonormal $n\times p$ matrices.

As in the unweighted case we can combine the two ALS
steps into a one-step upgrade for $T$.
$$
T^{(k+1)}=\Pi_T(CWT^{(k)}(\{T^{(k)}\}'WCWT^{(k)})^{-1/2})
$$




$$
\text{tr}\ (X-YT')W(X-YT')'=\text{tr}\ XWX'+\text{tr}\ T'WT-2\ \text{tr}\ Y'XWT
$$
$$
(F U)= procrustus(XW(A+D))
$$

## Statistical Considerations

$$
\underline{x}_{ij}=\mathcal{N}(\mu_j+\sum f_{is}a_{js}+u_{ij}d_j, s^2)
$$

The consistency and asymptotic normality of the MDFA estimates
has recently been studied by @terada_25.

Now define $C:=X'X$ and for any square symmetric T use the notation
$\lambda_s(T)$ for the ordered eigenvalues of $T$. Define
$$
\sigma(A,C):=\text{tr}\ C+\text{tr}\ A'A-2\sum_{s=1}^m\sqrt{\lambda_s(A'CA)}
$$
Differentiate this with repect to $A$, assuming the eigenvalues
are all different.
$$
\mathcal{D}_1\sigma(A,C)=2\{A-CA(A'CA)^{-\frac12}\}
$$
Note that $\mathcal{D}_1\sigma(A,C)=0$ if $C=AA'$.

1. The minimizer $A(C)$ is a continuous function of $C$.
2. The minimizer $A(C)$ is a differentiable function of $C$.


1. $\underline{C}_n$ converges weakly to $\Gamma=AA'$.
2. $\smash{n^{-\frac12}(\underline{C}_n-\Gamma)}$ is asymptotically normal.


\sectionbreak


# Code


# References
