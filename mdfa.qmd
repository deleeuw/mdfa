---
title: Matrix Decomposition Factor Analysis
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
suppressPackageStartupMessages(library(microbenchmark, quietly = TRUE))
suppressPackageStartupMessages(library(ggplot2, quietly = TRUE))
source("mdfaAuxiliary.R")
source("mdfaAlgorithmA.R")
source("mdfaAlgorithmB.R")
source("mdfaAlgorithmG.R")
source("emmett.R")
source("maxwell.R")
source("cattell.R")
source("bfi.R")
```

\sectionbreak

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All qmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://www.github.com/deleeuw/mdfa> 

\sectionbreak

# Introduction

Suppose $X$ is an $n\times m$ "tall" data matrix ($n\geq m$). 
We say that $X=YT'$, with $Y$
an $n\times p$ matrix and $T$ an $m\times p$ matrix, is a 
*decomposition of order $p$* of $X$. Decompositions of various kinds play a key role in theoretical and numerical linear algebra (@stewart_98). 

In this paper we are interested in the *factor analytic decomposition of order $p$*,
which is characterized by $m\leq p\leq n$ and by the requirement $Y'Y=I$. In addition there may be constraints on $T$, which we write in the general form $T\in\mathcal{T}$, with $\mathcal{T}$ a subset of $\mathbb{R}^{m\times p}$, the space of all $m\times p$ matrices.
Note that $Y$ is a "tall" matrix ($n\geq p$), while $T$ is "wide" ($m\leq p$).

Factor analysis techniques aim to find a solution of the system
\begin{subequations}
\begin{align}
X&=YT',\label{eq-fa1}\\
Y'Y&=I,\label{eq-fa2}\\
T&\in\mathcal{T}.\label{eq-fa3}
\end{align}
\end{subequations}
If no exact solution exists an approximate solution must be found.
Mathematically this introduces the problem to find the conditions
under which equations \eqref{eq-fa1}-\eqref{eq-fa3} can be solved for $Y$ and $T$, 
and to describe the set of solutions if the system is solvable.
Computationally the problem is to define what is meant by 
"approximately" and to find a technique that produces an
approximate solution. Typically this is done by defining a
non-negative loss function that measures departure from perfect fit and
an algorithm for minimizing it. 

In *Common Factor Analysis (CFA)*, which is the most important special case of 
factor analytic decomposition, the set $\mathcal{T}$ is the set of
partitioned matrices $\begin{bmatrix}A&\mid&D\end{bmatrix}$. The
matrix of *common factor loadings* $A$ is $m\times q$, with\footnote{The symbol $:=$ is used for definitions.} $q:=p-m$, and the matrix of *unique factor loadings* $D$ is diagonal of order $m$. In CFA parlance the diagonal elements of $D^2$ are called *uniquenesses*. In addition there can be constraints on $A$, which we write as $A\in\mathcal{A}$. If $A$ is unrestricted the CFA is *exploratory*, otherwise it is *confirmatory*. There is a corresponding partition of $Y$ as $\begin{bmatrix}F&\mid&U\end{bmatrix}$,
with $F$ the $n\times q$ matrix of *common factor scores* and $U$ the $n\times m$ matrix
of *unique factor scores*. 

For CFA the system \eqref{eq-fa1}-\eqref{eq-fa3} thus becomes
\begin{subequations}
\begin{align}
X&=FA'+UD,\label{eq-cfa1}\\
F'F&=I,\label{eq-cfa2}\\
U'U&=I,\label{eq-cfa3}\\
F'U&=0,\label{eq-cfa4}\\
D&=\text{diag(D)},\label{eq-cfa5}\\
A&\in\mathcal{A}.\label{eq-cfa6}
\end{align}
\end{subequations}

Although CFA will always be in the back of our mind, we will develop equations and algorithms
for the general case $X=YT$ with $T\in\mathcal{T}$.

\sectionbreak

# The Fundamental Theorem

In this section we look at finding exact solutions of the "full" system of equations
\eqref{eq-fa1}-\eqref{eq-fa3}. This shows under what conditions we can expect
perfect fit and a zero value for the minimum of our loss function. As we will see, 
the basic solvability result, which we call the "Fundamental Theorem of Factor
Analysis" following @kestelman_52, also has computational consequences.

We start by considering the "reduced" system, using $C:=X'X$,
\begin{subequations}
\begin{align}
C&=TT',\label{eq-pfa1}\\
T&\in\mathcal{T}.\label{eq-pfa2}
\end{align}
\end{subequations}
Solvability of the reduced system is a necessary condition for solvability
of the full system \eqref{eq-fa1}-\eqref{eq-fa3}.

Most factor analysis procedures are two-step methods. In the first step they find
an approximate solution $T$ to the reduced system, and then use this 
$T$ to find an approximate solution $Y$ to the full system. The two steps may 
actually use two different loss functions, the first one to assess 
the fit of $C=TT'$ and the second one the fit of $X=YT'$. This practice is motivated, and to some extent justified, by the following theorem, first
proved by @garnett_19.

::: {#thm-fundamental}

## Fundamental Theorem of Factor Analysis

The *full* system \eqref{eq-fa1}-\eqref{eq-fa3} is solvable if and only if the *reduced* system \eqref{eq-pfa1}-\eqref{eq-pfa2} is solvable.
:::
::: {.proof}
Necessity is trivial. For sufficiency we have to prove that if $T$ is any solution of the
reduced system then there is a $Y$ such that $(T,Y)$ satisfies the full system.
Our proof uses the "fat" singular value decomposition (SVD) of the $n\times m$ matrix $X$, assumed to be of rank $r$, which is
\begin{equation}
X_{n\times m}=
\begin{bmatrix}
K_{n\times r}^1&K^0_{n\times(n-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(m-r)}\\
0_{(n-r)\times r}&0_{(n-r)\times(m-r)}
\end{bmatrix}
\begin{bmatrix}
L_{m\times r}^1&L_{m\times(m-r)}^0
\end{bmatrix}'.\label{eq-fatx}
\end{equation}
The singular values in $\Lambda_{r\times r}$ are positive and decrease along the diagonal. Subscripts are used to indicate the dimension of the matrices.

From $C=TT'$ we know that a "fat" SVD of $T$ is
\begin{equation}
T_{m\times p}=
\begin{bmatrix}
L^1_{m\times r}&L^0_{m\times(m-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(p-r)}\\
0_{(m-r)\times r}&0_{(m-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',\label{eq-fatt}
\end{equation}
for some square orthonormal $M$. 

Write $Y$ as
\begin{equation}
Y_{n\times p}=
\begin{bmatrix}
K^1_{n\times r}&K^0_{n\times(n-r)}
\end{bmatrix} 
\begin{bmatrix}
S^{11}_{r\times r}&S^{10}_{r\times (p-r)}\\S^{01}_{(n-r)\times r}&S^{11}_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',\label{eq-writey}
\end{equation}
where the partitioned matrix $S$ in the middle must satisfy $S'S=I$.

Now $X=YT'$ becomes
\begin{multline}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(m-r)}\\
0_{(n-r)\times r}&0_{(n-r)\times(m-r)}
\end{bmatrix}=
\\
\begin{bmatrix}
S^{11}_{r\times r}&S^{10}_{r\times (p-r)}\\S^{01}_{(n-r)\times r}&S^{11}_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times (m-r)}\\
0_{(p-r)\times r}&0_{(p-r)\times(m-r)}
\end{bmatrix},\label{eq-prodyt}
\end{multline}
which implies  
\begin{equation}
S=\begin{bmatrix}
I_{r\times r}&0_{r\times(p-r)}\\
0_{(n-r)\times r}&V_{(n-r)\times(p-r)}
\end{bmatrix}\label{eq-solves}
\end{equation}
where $V$ satisfies $V'V=I$, but is otherwise arbitrary.

From \eqref{eq-writey} and \eqref{eq-solves} we see that
\begin{equation}
Y=K_1M_1'+K_0VM_0'\label{eq-yperf}
\end{equation}
provides us with the required solution of the full system.
:::

If the non-zero singular values of $X$, and consequently of $T$,  are all different then $K_1$ and 
$M_1$ are uniquely determined. Matrices $K_0$ and 
$M_0$ consist of orthonormal bases for the null-spaces of $X$ and $T$,
which are only determined up to rotations. We can select any one of these bases
and absorb the rotations in the arbitrary matrix $V$.
The fundamental theorem implies that for any solution $T\in\mathcal{T}$ of $C=TT'$ there is a linear subspace of solutions $Y$ of $X=YT'$. Even if $T$ is identified, $Y$ is not. This is the *factor indeterminacy problem*, which has haunted the field for more than 100 years.
(@steiger_schoenemann_78).

\sectionbreak

# Least Squares Factor Analysis

## MINRES

There are several factor analytic decomposition techniques that use least squares loss functions\footnote{We use SSQ() for the unweighted sum of squares of a matrix or vector.}. The oldest and most obvious one is
\begin{equation}
\sigma(T)=\frac12\text{SSQ}(C-TT'),\label{eq-minres}
\end{equation}
where $\text{SSQ}()$ stands for the unweighted sum of squares.
For CFA \eqref{eq-minres} becomes
\begin{equation}
\sigma(T)=\frac12\text{SSQ}(C-AA'-D^2).\label{eq-minrescfa}
\end{equation}
An alternating least squares technique to minimize \eqref{eq-minrescfa} was first proposed by @thomson_34. It alternates minimizing the loss function \eqref{eq-minrescfa} over $D$ for the current $A$ and minimizing over $A$ for the current $D$. Of course this only finds an approximate solution to the reduced system, and it leaves open the question on how to compute the factor scores $Y$. Alternative algorithms for minimizing \eqref{eq-minrescfa}
are MINRES of @harman_jones_66 and the Newton-Raphson technique of @derflinger_69 and @joreskog_vanthillo_71.

Also for CFA @young_40 and @whittle_52 propose minimizing the weighted least 
squares loss function
\begin{equation}
\sigma(A, D, F)=\frac12\text{tr}\ (X-FA')D^{-2}(X-FA')',\label{eq-young-whittle}
\end{equation}
which has the disadvantage that it assumes $D$ is known, or at least that there is a reasonable prior estimate of $D$. Minimizing \eqref{eq-young-whittle} has the advantage that the problem becomes a form of principal component analysis (PCA), in which the solution can be computed with a single SVD.

It is tempting to use block relaxation to minimize \eqref{eq-young-whittle}, alternating finding the optimum
$F$ and $A$ for fixed $D$, and then repeating this iteratively with the new $D$ equal to
\begin{equation}
D^2=n^{-1}\text{diag}\ (X-FA')'(X-FA').\label{eq-yw}
\end{equation}
This in fact corresponds to alternating minimization of the loss function
\begin{equation}
\sigma(A,F,D)=n\log\text{det}(D^2)+\text{tr}\ (X-FA')D^{-2}(X-FA')'.\label{eq-lawley41}
\end{equation}
But \eqref{eq-lawley41} is the negative likelihood loss function for the fixed factor model from @lawley_41, and we know this loss function is unbounded below, does not have a minimum, and converges to a perfect but trivial solution which has $p$ of the uniquenesses in $D^2$ equal to zero (@anderson_rubin_56).

The file lsfa.R in the repository has a function lsfa() for MINRES factor analysis. It
needs an initial estimate of $D$, for which we can take $\text{diag}^{-1}(C^{-1})$.
Then it applies a small number of Thomson iterations before switching to Newton
iterations. First and second derivatives are taken from @deleeuw_E_25d. Eigenvalue/eigenvector calculations use the RSpectra package (@qiu_mei_24).

## MDFA

A more direct way of fitting the full CFA model was discussed (independently and around the same time) in the dissertation of @socan_03 and in the conference chapter of @deleeuw_C_04a (presented at the conference in 2002). Recently this technique has become known as Matrix Decomposition Factor analysis (MDFA). Socan attributes MDFA (which they call "Direct Factor Analysis") to a 2001 personal communication and some unpublished notes of his advisor Henk Kiers.

In MDFA the least squares loss function is
\begin{equation}
\sigma(Y,A):=\frac12\text{SSQ}(X-YT'),\label{eq-lossdef}
\end{equation}
which must be minimized over $Y'Y=I$ and $T\in\mathcal{T}$.
MDFA was not immediately accepted as an alternative factor analysis technique. It made its first journal appearance in a series of papers by Unkel and Trendafilov, based largely on Unkel's dissertation (@unkel_09). Over the years they contributed a "robust"
version of MDFA (@unkel_trendafilov_10) and a version for a "wide" data matrix $X$ (@trendafilov_unkel_11). There is a nice review of these contributions in @unkel_trendafilov_10, with an update in @trendafilov_unkel_krzanowski_13.

There have been additional important contributions to MDFA in @adachi_12, @adachi_trendafilov_18,
@stegeman_16, @terada_25, @yamashita_25. We will discuss these recent contributions in various places in our present paper. When googling MDFA, keep in mind that Adachi initially used "Data-Fitting Factor Analysis", while Stegeman used the "Data Factor Model". 


\sectionbreak

# MDFA Algorithms

## Algorithm A

The Alternating Least Squares (ALS) algorithm proposed for MDFA by both Kiers (in @socan_03) and @deleeuw_C_04a alternates finding the optimum $Y$ for given $T$ and the optimum $T$ for given $Y$.

Finding the optimum $T$ for given $Y$ is  straightforward.
We complete the square, as in
\begin{equation}
\sigma(Y,T)=
\frac12\text{tr}\ C+\frac12\text{tr} (T-X'Y)'(T-X'Y)-\frac12\text{tr}\ Y'CY.
\end{equation}
Thus the optimum $T$ for given $Y$ is obtained by projecting 
$X'Y$ on the set of matrices $\mathcal{T}$. 

For exploratory CFA this gives 
$$
T=\begin{bmatrix}A&\mid&D\end{bmatrix}=\begin{bmatrix}X'F&\mid&\text{diag}(X'U)\end{bmatrix}.
$$ 
For confirmatory MDFA in which we require some loadings to be equal to each other and/or others to be equal to given constants finding $T$ is still a simple linear least squares problem. If there are inequality constraints we need some form of quadratic programming.

Also
\begin{equation}
\sigma(T,Y)=\frac12\text{tr}\ C+\frac12\text{tr}\ T'T-\text{tr}\ Y'XT.\label{eq-sigmay}
\end{equation}
We have to maximize $\text{tr}\ Y'XT$ over all
$n\times p$ orthonormal $Y$.
We start with some general lemmas. Remember that the *trace norm* (a.k.a. the 
*nuclear norm*) $\|Z\|_\star$ of a matrix $Z$ is the sum of its singular values. The trace norm is also equal to the sum of the square roots of the eigenvalues of $Z'Z$ and thus equal to the trace of $(Z'Z)^{1/2}$.

::: {#lem-procrustus}
If $X$ and $Y$ are $n\times p$ with $Y'Y=I$ then 
\begin{equation}
\text{tr}\ Y'X\leq\|X\|_\star.\label{eq-tnorm}
\end{equation}
:::

:::{.proof}
The stationary equations are $X=YM$ with $Y'Y=I$ and $M$ a symmetric matrix of Lagrange multipliers of order $p$. It follows that $M^2=X'X$ and thus $M$ is the (unique) positive semi-definite symmetric square root $\smash{(X'X)^{1/2}}$
At the maximum $X=YM$ implies $\text{tr}\ Y'X=\text{tr}\ M=\|X\|_\star$.
:::

Note that @lem-procrustus does not say that the optimizing $Y$ is unique. We get more insight from an alternative and more constructive proof (also taken from @deleeuw_C_04a, Appendix). It uses the "fat" singular value decomposition of the $n\times p$ matrix $X$ of rank $r$.

::: {#lem-construct}
The $n\times p$ orthonormal $Y$ maximizes $\text{tr}\ Y'X$, with 
\begin{equation}
X=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}\Lambda&0\\0&0\end{bmatrix}
\begin{bmatrix}L'\\L_\perp'\end{bmatrix},\label{eq-xa}
\end{equation}
if and only if
\begin{equation}
Y=KL'+K_\perp^{\ }SL_\perp',\label{eq-yopt}
\end{equation}
for some $(n-r)\times(p-r)$ matrix $S$ with $S'S=I$.
:::

::: {.proof}
Partition the columnwise orthonormal $n\times p$ matrix 
$Y$ in the same way as $X$.
\begin{equation}
Y=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}P&Q\\R&S\end{bmatrix}
\begin{bmatrix}L'\\L_\perp'\end{bmatrix},\label{eq-odecomp}
\end{equation}
where the four-block partitioned matrix in the middle of \eqref{eq-odecomp} must be orthonormal. Now $\text{tr}\ Y'X=\text{tr}\ \Lambda P$ and the constraint on the $r\times r$ matrix $P$ is that $P'P\lesssim I$ in the Loewner\footnote{$A\lesssim B$ means $A-B$ positive semi-definite.} sense (because we must have
$P'P+R'R=I$). It follows that the diagonal elements of $P$ must be less than or equal to one, and thus the maximum of
$\text{tr}\ Y'X$ is $\text{tr}\ \Lambda$, uniquely attained for $P=I$.
But $P=I$ implies that at the maximum $R=0$ and $Q=0$, and $S$ is any
$(n-r)\times(p-r)$ matrix with $S'S=I$. Thus the optimum $Y$ is
as given by \eqref{eq-yopt}.
:::

We can now apply the lemmas to minimizing the loss in \eqref{eq-sigmay}.

::: {#thm-procrustus}
If the $n\times p$ matrix $XT$ has rank $m$ with $m<p<n$ and "fat" singular value decomposition
\begin{equation}
XT=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}\Lambda&0\\0&0\end{bmatrix}
\begin{bmatrix}L'\\L_\perp'\end{bmatrix},\label{eq-xta}
\end{equation}
then the maximum of $\text{tr}\ Y'XT$ over $n\times p$ orthonormal $Y$ is 
$\|XT\|_\star$, attained for all $Y$ with
\begin{equation}
Y=KL'+K_\perp SL_\perp',\label{eq-yoptt}
\end{equation}
where $S$ is any
$(n-r)\times(p-r)$ matrix with $S'S=I$
:::
::: {.proof}
Directly from @lem-construct.
:::

It is important to note that in MDFA we cannot take $Y=KL'$, because then $Y'Y=LL'$, which is of rank $r\leq m$ and thus not equal to the identity if $r<p$. Also note that formula \eqref{eq-yoptt} is identical to formula \eqref{eq-yperf} in the fundamental theorem. This is not surprising, since both compute the optimal $Y$ for given $T$. The fundamental theorem
merely adds that the optimal $Y$ makes loss equal to zero if $C=TT'$.

It factor analytic literature it is often said that the factor scores $Y$ have the
"determinate" part $KL'$ and the "indeterminate" part $K_\perp SL_\perp'$. There
are some simple rank conditions which imply that the indeterminate part does not play a
role in Algorithm A, which uses $X'Y$ to update $T$,  and in the fitted values, which uses $YT'$.

::: {#cor-unique}

If $XT$ has rank $m$ and $Y$ is given by \eqref{eq-yoptt} then

1. $X'Y=X'KL'=CT(T'CT)^{-1/2}$. 

2. $YT'=KL'T'=XT(T'CT)^{-1/2}T'$.

3. $T'X'Y=(T'CT)^{1/2}$.
:::
::: {.proof}
If $XT$ is of rank $m$ then both $X$ and $T$ are of rank $m$.
We know that $K_\perp'XT=0$. Because $\text{rank}(T)=m$ this implies $X'K_\perp=0$.
This proves part one.

Because $\text{rank}(X)=m$ and $XTL_\perp=0$ we have $L_\perp'T=0$. This proves part 
two. 

Part three follows directly from part one. 
:::


The CFA equations \eqref{eq-cfa1}-\eqref{eq-cfa6} imply that $0\lesssim AA'\lesssim C$ and $0\lesssim D^2\lesssim C$. The second one in turn implies that $0\lesssim D^2\lesssim\text{diag}(C)$. In most CFA techniques it is possible that the estimated
uniquenesses do not satisfy these inequality constraints
and are consequently "improper" solutions. This is true for both MINRES and ML. We
show that it is "less true" for MDFA.

::: {#thm-bounds}
In the exploratory MDFA solution $0\lesssim AA'\lesssim C$ and 
$0\lesssim D^2\lesssim\text{diag}(C)$.
:::
::: {.proof}
At the solution we have $A=X'F$ and thus $AA'=X'FF'X$.
Because $FF'\lesssim I$ we have $AA'\lesssim C$. At the
solution we also have $D=\text{diag}(X'U)$, or
$d_j=x_j'u_j$. Since $u_j'u_j=1$  we have from Cauchy-Schwartz 
$d_j^2\leq x_j'x_j=c_{jj}$.
:::

@thm-bounds shows that worst-case improper solutions 
do not happen in MDFA. Uniquenesses are always non-negative
and bounded above by the diagonal of $C$, i.e. for correlation 
matrices they are always between zero and one.
Note however that we have not shown that $D^2\lesssim C$,
so not all improper solutions are excluded.

The repository has a file mdfaAlgorithmA.R, with the function mdfaAlgorithmA() implementing
Algorithm A. There is a function mdfaProject() which take care of the least squares projections on $\mathcal{T}$. By default mdfaProject() replaces the matrix with the last $m$ columns of $T$ by its diagonal, but replacing the default projection routine allows us to incorporate any set of constraints. 

We emphasize that mdfaAlgorithmA() updates $Y$
using the determinate part $KL'$ only, which means that in our iterations we do not have $Y'Y=I$. @cor-unique show that this does not impact the sequences of loss function values and of iterates for $T$, but it does mean that if we want a proper $Y$ then we have to do some extra work after convergence. @adachi_12, section 2.5, makes a case for using only the determinate part, but it remains true that this does not solve the original MDFA problem, which requires $Y'Y=I$. Also computing a proper $Y$ corresponding with the optimal $T$
forces us to make a choice of $S$ in \eqref{eq-yoptt}.




## Algorithm B

Write $\Pi_Y()$ for the least squares projection of
a matrix on the columnwise orthonormal matrices of the same
dimension. Also write $\Pi_T()$ for least squares
projection on $\mathcal{T}$. Then ALS iteration $k$ of Algorithm A is
\begin{subequations}
\begin{align}
Y^{(k)}&=\Pi_Y(XT^{(k)}),\label{eq-als1}\\
T^{(k+1)}&=\Pi_T(X'Y^{(k)}).\label{eq-als2}
\end{align}
\end{subequations}
If we assume that $\text{rank}(X)=\text{rank}(T)=m$ then it  does not matter how we choose $S$ in \eqref{eq-yopt}, because by @cor-unique for all
choices of $S$ we have $\text{tr}\ Y'XT=\|XT\|_\star$, and all choices of $S$ give
the same update and the same loss function value.

In most factor analytic applications we have $n>>m$. The computations 
in \eqref{eq-als1} and \eqref{eq-als2} use the matrices $X$ and $Y$, which each
have $n$ rows. Every iteration involves multiplications with these
potentially very large matrices. This will tend to be 
expensive computationally. It has been pointed out by @adachi_12
that we can rewrite the ALS algorithm in such a way that it only 
involves matrices of order $m$. This alternative derivation of
the algorithm has the additional property that it can be
applied to $C$ without having to know $X$, which has obvious
advantages for secondary analysis.

::: {#thm-als}
Suppose $T^{(k)}$ has rank $m$. Then
\begin{equation}
T^{(k+1)}=\Pi_T\{CT^{(k)}(T^{(k)}CT^{(k)})^{-1/2}\}.\label{eq-alsshort}
\end{equation}
:::
::: {.proof}
We rewrite \eqref{eq-yopt} as
$$
Y=XT(T'CT)^{-1/2}+K_\perp^{\ } SL_\perp'
$$
where $(T'CT)^{-1/2}$ is the square root of the Moore-Penrose inverse of 
$T'CT$, $K_\perp$ and $L_\perp$ are orthonormal bases for the null
spaces of $XT$, and $S$ is arbitrary.
$$
T^{(k+1)}=\Pi_T(X'Y)=\Pi_T\left\{CT^{(k)}(\{T^{(k)}\}'CT^{(k)})^{-\frac12}+X'K_\perp^{\ } SL_\perp'\right\}
$$
We have $K_\perp'XT=0$. If $T$ has rank $m$ then this implies
$X'K_\perp=0$ which leads to the result in the theorem.
:::

Algorithm \eqref{eq-alsshort} requires us to compute $(T'CT)^{-\frac12}$ in each iteration, which means finding the $m$ largest eigenvalues and their corresponding eigenvectors. The update is then projected on the set $\mathcal{T}$ defined by the template. The ALS interpretation of the
algorithm shows directly that a decreasing sequence of loss function values is produced, which
proves convergence of $T$ under the usual identification and rank conditions.

In Algorithm A we only compute the determinate part of $Y$, in Algorithm B there is no $Y$
at all. In both cases finding appropiate (indeterminate) factor scores requires additional work, not unlike what must be done in the MINRES or ML techniques for CFA. 

## Algorithm C

In @deleeuw_C_04a an alternative ALS algorithm for CFA
is mentioned and briefly described. It minimizes the CFA loss
\begin{equation}
\sigma(F,A,U,D)=\frac12\text{SSQ}(X-FA'-UD),\label{eq-cfaloss}
\end{equation}
with the constraints $F'F=I$, $U'F=0$, $U'U=I$, $A\in\mathcal{A}$, and $D\in\mathcal{D}$.
Write $\Pi_A()$ and $\Pi_D()$ for the least squares projectors on $\mathcal{A}$ and
$\mathcal{D}$. 

In Algorithm A we use a two-block ALS strategy. We alternate the update of $Y=(F\mid U)$ for given $T=(A\mid D)$ and of $T=(A\mid D)$ for given $Y=(F\mid U)$. In Algorithm $C$ we use
a four-block ALS strategy. The four steps in an ALS
iteration of Algorithm C are updating $F$ for given $(A, U, D)$, $U$ for given $(F,A,D)$,  $A$ for given $(F, U, D)$, and $D$ for given $(A,F,U)$. The third and fourth step are the same as in Algorithm A, the first two steps are different.

Start the ALS cycle with minimizing $\sigma()$ over $F$ for given $(A, U, D)$ with the constraints $F'F=I$ and $F'U=0$. This is equivalent to maximizing $\text{tr}\ F'XA$ over $F$ satisfying $F'F=I$ and $F'U=0$. Now $F'U=0$ if and only if $F=U_\perp V$,
where $U_\perp$ is an $n\times(n-m)$ orthonormal basis for the complement of $U$ and $V$ is any $(n-m)\times q$ matrix. Thus we must maximize $\text{tr}\ V'U_\perp'XA$ over $V$ satisfying $V'V=I$. The optimum $V$ is the projection of $U_\perp'XA$
on the set of orthonormal $(n-m)\times q$ matrices. We use the symbol $\leftarrow$
for updates in iterations. Thus $Z\leftarrow f(Z)$ means "the new $Z$ becomes 
function $f$ of the previous $Z$", which is the same thing as $Z^{(k+1)}=f(Z^{(k)})$,
but without the reference to a specific iteration. We have
\begin{subequations}
\begin{equation}
F\leftarrow U_\perp V=U_\perp^{\ } U_\perp'XA
(A'X'U_\perp^{\ } U_\perp'XA)^{-1/2}.\label{eq-fupdate}
\end{equation}
In the same way the optimum $U$ for given $(F,A,D)$ is
\begin{equation}
U\leftarrow F_\perp^{\ } F_\perp'XD
(DX'F_\perp^{\ } F_\perp'XD)^{-1/2}.\label{eq-uupdate}
\end{equation}
\end{subequations}
In each ALS cycle we update in the order $(F,A,U,D)$. Thus
\begin{equation}
A\leftarrow\Pi_A(X'F)=\Pi_A(X'U_\perp^{\ } U_\perp'XA
(A'X'U_\perp^{\ } U_\perp'XA)^{-1/2}).\label{eq-aupdate}
\end{equation}
Now use
$U_\perp^{\ } U_\perp'=I-UU'$ and define $\tilde D:=X'U$,
so that $D=\Pi_D(\tilde D)$. Then \eqref{eq-aupdate} becomes
\begin{subequations}
\begin{equation}
A\leftarrow \Pi_A\left\{(C-\tilde D\tilde D')A[A'(C-\tilde D\tilde D')A]^{-1/2}\right\}.\label{eq-aupdate2}
\end{equation}
In the same way, using  $F_\perp^{\ } F_\perp'=I-FF'$ and $\tilde A:=X'F$, gives the update
\begin{equation}
D\leftarrow\Pi_D(X'U)=\Pi_D\left\{(C-\tilde A\tilde A')D
[D(C-\tilde A\tilde A')D]^{-1/2}\right\}.\label{eq-dupdate}
\end{equation}
\end{subequations}

Although \eqref{eq-aupdate2} and \eqref{eq-dupdate} show we can iterate in Algorithm C without having to compute
$F_\perp$ and $U_\perp$ we still need $X$ to compute $\tilde A$ and $\tilde D$. Thus simplifications as in Algorithm B do not seem possible. This makes it unlikely that
Algorithm C will actually be useful for CFA computations. 

The CFA definition \eqref{eq-cfa1}-\eqref{eq-cfa6} implies $A=X'F$ and 
$D=X'U$. In an exploratory MDFA solution we do have
$A=X'F$ but $D=\text{diag}(X'U)$. As a consequence
unique factor $u_i$ in the solution can be correlated
with variables $x_j$, where $j\neq i$. This violates 
the very idea of unique factors. We may conclude that a non-diagonal $X'U$
indicates a lack of CFA fit. 
We may also find it sufficiently counter-intuitive and prevent 
it from happening.

In the thesis of Socan an ALS variation of MDFA is discussed in which it is required, in addition to $Y'Y=I$, that $U'X$ is diagonal. In the case of perfect fit the constraint $U'X=D$ is automatically satisfied, but it will not be so for the least squares fit. 

In the version of Algorithm C in which we also require that $U'X=\Omega$, with $\Omega$
a diagonal matrix with additional unknowns, we can update $A$, $D$, and $F$ as before.
But for $U$ and $\Omega$ we have to solve maximizing $\text{tr}\ U'XD$ over $U$ and $\Omega$
such that $U'F=0$, $U'U=I$, and $U'X=\Omega$. We satisfy $U'F=0$ and $U'U=I$ by
choosing $U=F_\perp V$ with $V'V=I$. It remains to maximize $\text{tr}\ V'F_\perp'XD$
over $V$ with $V'F_\perp'X=\Omega$ and $V'V=I$. 

Assume $F_\perp'X$ has rank $m$. Then the general solution of $V'F_\perp'X=\Omega$
is
$$
V=F_\perp' X(X'F_\perp F_\perp' X)^{-1}\Omega+HM.
$$
with $H$ a basis for the null space of $F_\perp'X$ and with $M$ arbitrary. For this $V$
we must have
$$
V'V=\Omega(X'F_\perp F_\perp' X)^{-1}\Omega+M'M=I
$$
which is solvable if and only if $\Omega$ satisfies $\Omega(X'F_\perp F_\perp' X)^{-1}\Omega\lesssim I$, or, $\Omega^2\lesssim C-\tilde A\tilde A'$. Thus 
our problem becomes maximizing $\text{tr}\ \Omega D$ under these constraints.
This problem, although convex, is not a simple least squares projection
problem, and we currently have no implementation.

## Algorithm G

The minimum of $\sigma(Y,T)$ over $Y'Y=I$ is
\begin{equation}
\sigma_\star(T):=\frac12\text{tr}\ X'X+\frac12\text{tr}\ T'T-\|XT\|_\star=\frac12\text{tr}\ X'X+\frac12\text{tr}\ T'T-\sum_{\nu=1}^m\lambda_\nu^\frac12(T'CT).\label{eq-projloss}
\end{equation}
First and second derivatives of the trace norm with respect to $T$ were computed in @deleeuw_E_25d. The gradient is
\begin{equation}
\mathcal{D}\sigma_\star(T)=T-CT(T'CT)^{-1/2}.\label{eq-projgrad}
\end{equation}
With this formula for the gradient many general purpose optimization methods for minimizing the projected loss function \eqref{eq-projloss} become available. The R function
mdfaAlgorithmG() in the repository uses the BFGS method of the optim() function from the stats package (@r_core_team_25). 

Compared with Algorithm B we can perhaps expect some gain in efficiency, because of the
superlinear convergence of the BFGS method, but we lose some flexibility by not having
the projections $\Pi_T()$ available.


## Algorithm H


The definition of $\mathcal{T}$ that was used is
$$
T=T_0+\sum_{s=1}^q\theta_s T_s,
$$
which allows for patterns with some elements fixed and some elements equal to other elements.
$$
\mathcal{D}_s\sigma_\star=\text{tr}\ T'T_s-\text{tr}\ (T'CT)^{-1/2}T'CT_s
$$

The derivatives of the loss function ,,, with respect to the parameters $\theta_s$ are most easily expressed by first computing the derivatives of the eigenvalues of $T'CT$.
\begin{equation}
\mathcal{D}_s\lambda_\nu=x_\nu'Q_sx_\nu,\label{eq-mdfadl}
\end{equation}
where $Q_s:=T_s'CT+T'CT_s$. It follows that
\begin{equation}
\mathcal{D}_{st}\lambda_\nu=-2x_\nu'Q_s(A-\lambda_\nu I)^+Q_tx_\nu+2x_\nu'T_s'CT_tx_\nu.\label{eq-mdfaddl}
\end{equation}


\sectionbreak

# Examples

## Emmett

Our example is the correlation matrix of order nine from 
@emmett_49, also used in @lawley_maxwell_71, pp. 42-44. We use three common factors, and compare our results with those obtained using multinormal maximum likelihood (@lawley_maxwell_71, table 4.2). We cannot apply algorithm A here, because we do not have 
the data matrix $X$, only the correlation matrix $C$.

```{r emmett, echo = FALSE}
matrixPrint(emmett, digits = 3, width = 5, flag = "")
```

```{r alga, echo = FALSE, cache = TRUE}
source("emmett.R")
h <- mdfaAlgorithmB(emmett, emtold, itmax = 1000, verbose = FALSE)
g <- mdfaConvertTtoAD(h$tmat)
```

As the initial estimate for $A$ we use the first three
principal components, scaled to the length of the corresponding eigenvalues. The initial estimate of $D$ is the square root of the diagonal of $I-AA'$. Algorithm B converges in `r h$itel` iterations to a loss of `r h$loss`, where convergence is defined
by a loss-decrease in successive iterations of less than 
`r 1e-10`. The MDFA uniquenesses are

```{r alga_d, echo = FALSE}
matrixPrint(g$uniquenesses, digits = 3, width = 5, flag ="")
```
The uniquenesses from the ML solution in Lawley and Maxwell's Table 4.2 are
```{r ml_d, echo = FALSE}
mld <- c(.450, .427, .617, .212, .381, .177, .400, .462, .231)
matrixPrint(mld, digits = 3, width = 5, flag ="")
```
Comparing loadings is slightly more involved. The ML solution
is rotated so that $A'D^{-2}A$ is diagonal, so we rotate the 
MDFA solution in the same way. The MDFA loadings are

```{r alga_a, echo = FALSE}
matrixPrint(-g$loadings, digits = 3, width = 5)
```

The ML loadings are
```{r ml_a, echo = FALSE}
ml_a <- matrix(c(
  .664, .689, .493, .837, .705, .819, .661, .458, .766,
  .321, .247, .302, -.292, -.315, -.377, .396, .296, .427,
  .074, -.193, -.222, -.035, -.153, .105, -.078, .491, -.012),
  9, 3)
matrixPrint(ml_a, digits = 3, width = 5)
```

The Emmett example is somewhat atypical, because the CFA model with three factors has a near perfect fit. This explains why the MDFA and ML solutions are nearly identical.

The AlgorithmB solution computed by Algorithm G requires 29 function evaluations and 27 gradient evaluations.


```{r}
source("emmett.R")
suppressWarnings(h <- microbenchmark(
  mdfaAlgorithmB(emmett, emtold, itmax = 1000, verbose = FALSE),
  factanal(NULL, factors = 3, covmat = emmett)
))
```




## Maxwell

Lawley and Maxwell give a second example, using data also used by
@maxwell_61 (ten variables, 810 observations). Thew cxorrelation matrix
is

```{r maxwell, echo = FALSE}
matrixPrint(maxwell, digits = 3, width = 5, flag = "")
```

```{r maxalgb, echo = FALSE, cache = TRUE}
h <- mdfaAlgorithmB(maxwell, matold, itmax = 10000, verbose = FALSE)
g <- mdfaConvertTtoAD(h$tmat)
```

Algorithm B converges in `r h$itel`
iterations to a loss of `r h$loss`. The MDFA uniquenesses are

```{r maxalgb_d, echo = FALSE}
matrixPrint(g$uniquenesses, digits = 3, width = 5, flag ="")
```
The uniquenesses from the ML solution in Lawley and Maxwell's Table 4.6 are
```{r maxml_d, echo = FALSE}
maxmld <- c(.385, .623, .301, .638, .347, .778, .286, .000, .690, .600)
matrixPrint(maxmld, digits = 3, width = 5, flag = "")
```
Although the uniquenesses are close, there is an important difference between
MDFA and ML. Obtaining the ML solution required manual intervention, because the
four-factor solution was "improper". The uniqueness of variable 8 was judged to 
be a Heywood case, and the analysis with repeated without variable 8. The final results
identify the first factor with variable 8, so the loadings on the first factor
are just the corresponding column of the correlation matrix. MDFA gives the same
results without any manual intervention. The only consequence of the Heywood case is that more iterations are required to drive the uniqueness to zero.

The MDFA loadings are, after suitable rotation,

```{r maxalgb_a, echo = FALSE}
matrixPrint(g$loadings, digits = 3, width = 5)
```

The ML loadings are
```{r maxml_a, echo = FALSE}
maxml_a <- matrix(c(
  .188, .120, .186, .173, .129, .359, .448, 1.000, .429, .316,
  .753, .468, .767, .526, .672, .259, .504, .000, .282, .232,
  -.035, -.103, -.167, -.200, -.251, .154, .507, .000, .209, .496,
  -.108, .365, .217, .124, -.349, .048, -.0523, .000, .053, .029),
  10, 4)
matrixPrint(maxml_a, digits = 3, width = 5)
```

## Cattell

This example uses the correlation matris between 12 cognitive variables from @cattell_63,
included in the psych package (@revelle_25), to illustrate fitting a group factor model.
The variables 1 and 2 are verbal ability, 3 and 4 are spatial ability, 5 and 6 are reasoning, 7 and 8 are numerical ability. The first eight are from the Thurstone {Primary Abilities test, and the last four are from the (IPAT) Culture Fair Intelligence Test.
We apply Algorithm B with 6 factors: one general factor and five group factors. 

```{r cattell, echo = FALSE}
source("cattell.R")
h <- mdfaAlgorithmB(cattell, catold, cattellProjection, itmax = 1000, verbose = FALSE)
g <- mdfaConvertTtoAD(h$tmat, rotate = FALSE)
```
Algorithm B uses `r h$itel` iterations to converge to loss `r h$loss`. The loadings
are
```{r cattella, echo = FALSE}
matrixPrint(g$loadings)
```
and the uniquesnesses are
```{r cattelld, echo = FALSE}
matrixPrint(g$uniquenesses)
```
@rindskopf_rose_88 also do a group factor analysis, but they allow for correlations
between the five group factors. This makes their results not directly comparable to 
ours.



## BFI

In this example we use the BFI data from the psych package (@revelle_25). There are 2800
subjects and 25 personality self report items. We use the impute() function from the
e1071 package, with the mean option, to fill in the missing data (@meyer_dimitriadou_hornik_weingessel_leisch_24). In this example we can use
mdfaAlgorithmA(), because the complete data matrix is available.
```{r bfi, echo = FALSE, cache = TRUE}
ha <- mdfaAlgorithmA(bfi, bftold, verbose = FALSE)
hb <- mdfaAlgorithmB(cfi, bftold, verbose = FALSE)
```
Algorithms A and B both require `r ha$itel` iterations to arrive at the same solution with loss `r ha$loss`.

```{r bfibench, echo = FALSE, cache = TRUE}
suppressWarnings(h <- microbenchmark(mdfaAlgorithmA(bfi, told, verbose = FALSE), mdfaAlgorithmB(cfi, told, verbose = FALSE)))
```
The median execution time, as measured by the microbenchmark packages (@mersmann_24), is 435.936128 milliseconds for Algorithm A and 5.39906 milliseconds for Algorithm B, which is consequently about 80 times faster. The 100 benchmark runs are plotted below.
```{r bfibenchplot, echo = FALSE, fig.align = "center", fig.cap = "Comparison Algorithms A and B"}
autoplot(h)
```


\sectionbreak

# Discussion



## Correlated Factors and Errors


## Scale Freeness

An MDFA procedure $X\rightarrow(Y,T)$ maps data matrices
$X$ to a scores $Y$ and loadings $T$. The
procedure is scale-free if for all diagonal $\Phi$
we have $X\Phi\rightarrow(Y,T\Phi)$. The MDFA least squares
techniques we have discussed so far are not scale-free, but 
they can be made scale-free in various ways.

The first way is traditional. We normalize the data
in $X$ to unit length, so that $C=X'X$ becomes a
correlation matrix. This can be formalized as 
minimizing
$$
\sigma_W(T,Y):=\text{tr}\ (X-YT')W(X'-TY')
$$
with $W:=\text{diag}^{-1}(C)$. @adachi_12
proposes instead to use $\smash{W=C^{-1}}$.
But no matter how we choose the positive definite
$W$ the computational consequences are
easy to handle.

We have
$$
\sigma_W(T,Y)=\text{tr}\ XWX'+\text{tr}\ (T-X'Y)'W(T-X'Y)-\text{tr}\ Y'XWX'Y
$$
and consequently the optimal $T$ for given $Y$ is the
projection in the metric $W$ of $X'Y$ on $\mathcal{T}$.
This may or may not be more complicated than unweighted
projection, depending on the definition of $\mathcal{T}$.

Also
$$
\sigma_W(T,Y)=\text{tr}\ XWX'+\text{tr}\ T'WT-2\ \text{tr}\ Y'XWT,
$$
which means the optimal $Y$ for given $T$ is the 
(unweighted) projection of $XWT$ on the orthonormal 
$n\times p$ matrices.

As in the unweighted case we can combine the two ALS
steps into a one-step upgrade for $T$.
$$
T^{(k+1)}=\Pi_T(CWT(\{T^{(k)}\}'WCWT^{(k)})^{-1/2})
$$

## Uniqueness


## Statistical Considerations

$$
\underline{x}_{ij}=\mathcal{N}(\mu_j+\sum f_{is}a_{js}+u_{ij}d_j, s^2)
$$

The consistency and asymptotic normality of the MDFA estimates
has recently been studied by @terada_25.

Now define $C:=X'X$ and for any square symmetric T use the notation
$\lambda_s(T)$ for the ordered eigenvalues of $T$. Define
$$
\sigma(A,C):=\text{tr}\ C+\text{tr}\ A'A-2\sum_{s=1}^m\sqrt{\lambda_s(A'CA)}
$$
Differentiate this with repect to $A$, assuming the eigenvalues
are all different.
$$
\mathcal{D}_1\sigma(A,C)=2\{A-CA(A'CA)^{-\frac12}\}
$$
Note that $\mathcal{D}_1\sigma(A,C)=0$ if $C=AA'$.

1. The minimizer $A(C)$ is a continuous function of $C$.
2. The minimizer $A(C)$ is a differentiable function of $C$.


1. $\underline{C}_n$ converges weakly to $\Gamma=AA'$.
2. $\smash{n^{-\frac12}(\underline{C}_n-\Gamma)}$ is asymptotically normal.


\sectionbreak


# References
