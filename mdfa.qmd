---
title: Matrix Decomposition Factor Analysis
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
source("mdfaOptim.R")
matrixPrint <- function(x,
                        digits = 6,
                        width = 8,
                        format = "f",
                        flag = "+") {
  print(noquote(
    formatC(
      x,
      digits = digits,
      width = width,
      format = format,
      flag = flag
    )
  ))
}
```

\sectionbreak

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All qmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/mdfa> 

\sectionbreak

# Introduction

Suppose $X$ is an $n\times m$ "tall" data matrix ($n\geq m$). 
We say that $X=YA'$, with $Y$
an $n\times p$ matrix and $A$ an $m\times p$ matrix, is a 
*decomposition of order $p$* of $X$. Decompositions of various kinds play a key role in theoretical and numerical linear algebra (@stewart_98). 

In this paper
we are interested in the *factor analytic decomposition of order $p$*,
which is characterized by $m\leq p\leq n$ and by the requirement $Y'Y=I$. In addition there may be constraints on $A$, which we write in the general form $A\in\mathcal{A}$, with $\mathcal{A}$
a subset of $\mathbb{R}^{m\times p}$, the space of all $m\times p$ matrices.
Note that $Y$ is a "tall" matrix ($n\geq p$), while $A$ is "wide" ($m\leq p$).

Factor analysis techniques aim to find a solution of the system
\begin{subequations}
\begin{align}
X&=YA',\label{eq-fa1}\\
Y'Y&=I,\label{eq-fa2}\\
A&\in\mathcal{A}.\label{eq-fa3}
\end{align}
\end{subequations}
If no exact solution exists an approximate solution must be found.
Mathematically this introduces the problem to find the conditions
under which equations \eqref{eq-fa1}-\eqref{eq-fa3} can be solved for $Y$ and $A$, 
and to describe the set of solutions if the system is solvable.
Computationally the problem is to define what is meant by 
"approximately" and to find a technique that produces an
approximate solution. Typically this is done by defining a
non-negative loss function that measures departure from perfect fit and
an algrotihm for minimizing it. 

In *Common Factor Analysis (CFA)*, which is the most important special case of 
factor analytic decomposition, the set $\mathcal{A}$ is the set of
partitioned matrices $\begin{bmatrix}F'&\mid&D\end{bmatrix}$. The
matrix of *common factor loadings* $F$ is $m\times q$, with\footnote{The symbol $:=$ is used for definitions.} $q:=p-m$, and the matrix of *unique factor loadings* $D$ is diagonal of order $m$. In CFA parlance the diagonal elements of $D^2$ are called *uniquenesses*. In addition there can be constraints on $F$, which we write as $F\in\mathcal{F}$. If $F$ is unrestricted the CFA is *exploratory*, otherwise it is *confirmatory*. There is a corresponding partition of $Y$ as $\begin{bmatrix}T&\mid&U\end{bmatrix}$,
with $T$ the $n\times q$ matrix of *common factor scores* and $U$ the *m\times m* matrix
of *unique factor scores*. 

For CFA the system \eqref{eq-fa1}-\eqref{eq-fa3} thus becomes
\begin{subequations}
\begin{align}
X&=TF'+UD',\label{eq-cfa1}\\
T'T&=I,\label{eq-cfa2}\\
U'U&=UU'=I,\label{eq-cfa3}\\
T'U&=0,\label{eq-cfa4}\\
D&=\text{diag(D)},\label{eq-cfa5}\\
F&\in\mathcal{F}.\label{eq-cfa6}
\end{align}
\end{subequations}

\sectionbreak

# The Fundamental Theorem

In this section we look at finding exact solutions of the "full" system of equations
\eqref{eq-fa1}-\eqref{eq-fa3}. This shows under what conditions we can expect
perfect fit and a zero value for the minimum of our loss function. As we will see, 
the basic solvability result, which we call the "Fundamental Theorem of Factor
Analysis" following @kestelman_52, also has computational consequences.

We start by considering the "reduced" system
\begin{subequations}
\begin{align}
X'X&=AA',\label{eq-pfa1}\\
A&\in\mathcal{A}.\label{eq-pfa2}
\end{align}
\end{subequations}
Solvability of the rerduced system is a necessary condition for solvability
of the full system \eqref{eq-fa1}-\eqref{eq-fa3}.
Most factor analysis procedures are two-step methods. In the first step they find
an approximate solution $A$ to the reduced system, and then use this 
$A$ to find an approximate solution $Y$ to the full system. The two steps may 
actually use two different loss functions, the first one to assess 
the fit of $X'X=AA'$ and the second one the fit of $X=YA'$. This practice is motivated, and to some extent justified, by the following theorem, first
proved by @garnett_19.

::: {#thm-fundamental}

## Fundamental Theorem of Factor Analysis

The *full* system \eqref{eq-fa1}-\eqref{eq-fa3} is solvable if and only if the *reduced* system \eqref{eq-pfa1}-\eqref{eq-pfa2} is solvable.
:::
::: {.proof}
Necessity is trivial. For sufficiency we have to prove that if $A$ is any solution of the
reduced system then there is a $Y$ such that $(A,Y)$ satisfies the full system.
Our proof uses the "fat" singular value decomposition (SVD) of the $n\times m$ matrix $X$, assumed to be of rank $r$, which is
\begin{equation}
X_{n\times m}=
\begin{bmatrix}
K_{n\times r}^1&K^0_{n\times(n-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(m-r)}\\
0_{(n-r)\times r}&0_{(n-r)\times(m-r)}
\end{bmatrix}
\begin{bmatrix}
L_{m\times r}^1&L_{m\times(m-r)}^0
\end{bmatrix}'.
\end{equation}
The singular values in $\Lambda_{r\times r}$ are positive and decrease along the diagonal. Subscripts are used to indicate the dimension of the matrices.

From $X'X=AA'$ we know that a "fat" SVD of $A$ is
\begin{equation}
A_{m\times p}=
\begin{bmatrix}
L^1_{m\times r}&L^0_{m\times(m-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(p-r)}\\
0_{(m-r)\times r}&0_{(m-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',
\end{equation}
for some square orthonormal $M$. 

Write $Y$ as
\begin{equation}
Y_{n\times p}=
\begin{bmatrix}
K^1_{n\times r}&K^0_{n\times(n-r)}
\end{bmatrix} 
\begin{bmatrix}
U^{11}_{r\times r}&U^{10}_{r\times (p-r)}\\U^{01}_{(n-r)\times r}&U^{11}_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',
\end{equation}
where the partitioned matrix $U$ in the middle must satisfy $U'U=I$.

Now $X=YA'$ becomes
\begin{multline}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times(m-r)}\\
0_{(n-r)\times r}&0_{(n-r)\times(m-r)}
\end{bmatrix}=
\\
\begin{bmatrix}
U^{11}_{r\times r}&U^{10}_{r\times (p-r)}\\U^{01}_{(n-r)\times r}&U^{11}_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
\Lambda_{r\times r}&0_{r\times (m-r)}\\
0_{(p-r)\times r}&0_{(p-r)\times(m-r)}
\end{bmatrix}
\end{multline}
which implies  
\begin{equation}
U=\begin{bmatrix}
I_{r\times r}&0_{r\times(p-r)}\\
0_{(n-r)\times r}&V_{(n-r)\times(p-r)}
\end{bmatrix}
\end{equation}
where $V$ satisfies $V'V=I$, but is otherwise arbitrary.

From ... 
\begin{equation}
Y_{n\times p}=
\begin{bmatrix}
K^1_{n\times r}&K^0_{n\times(n-r)}
\end{bmatrix} 
\begin{bmatrix}
I_{r\times r}&0_{r\times (p-r)}\\0_{(n-r)\times r}&V_{(n-r)\times(p-r)}
\end{bmatrix}
\begin{bmatrix}
M^1_{p\times r}&M^0_{p\times(p-r)}
\end{bmatrix}',
\end{equation}

If the non-zero singular values of $X$, and consequently of $A$,  are all different then $K^1_{n\times r}$ and 
$M^1_{p\times r}$ are uniquely determined. Matrices $K^0_{n\times(n-r)}$ and 
$M^0_{p\times(p-r)}$ consist of orthonormal bases for the null-spaces of $X$ and $A$,
which are only determined up to rotations. We can select any one of these bases
and absorb the rotations in the arbitrary matrix $V_{(n-r)\times(p-r)}$.
:::

The fundamental theorem implies that for any solution $A\in\mathcal{A}$ of $X'X=AA'$ there is a manifold of solutions $Y$ of $X=YA'$. Even if $A$ is identified, $Y$ is not. This is the *factor indeterminacy problem*, which has haunted the field for more than 100 years.
(@steiger_schoenemann_78).

\sectionbreak

# Least Squares Factor Analysis

There are several factor analytic decomposition techniques that use least squares loss functions. The oldest one is, first in the general case and then for CFA,
\begin{subequations}
\begin{align}
\sigma(A)&=\text{SSQ}(X'X-AA'),\label{eq-minres1}\\
\sigma(F,D)&=\text{SSQ}(X'X-FF'-D^2).\label{eq-minres2}
\end{align}
\end{subequations}
An alternating least squares technique to minimize \eqref{eq-minres2} in the case of CFA was first proposed by @thomson_34. It alternates minimizing the loss function over $D$ for the current $F$ and minimizing over $F$ for the current $D$. Of course this only finds an approximate solution to the reduced system, and it leaves open the question on how to compute the factor scores $Y$. Alternative algorithms for minimizing \eqref{eq-minres2}
are MINRES of @harman_jones_66 and the Newton-Raphson technique of @derflinger_69.

For CFA @young_40 and @whittle_52 propose minimizing the weighted least 
squares loss function
\begin{equation}
\sigma(T,F)=\text{tr}\ (X-TF')\Delta^{-2}(X-TF')',\label{eq-young-whittle}
\end{equation}
which has the disadvantage that it assumes that $\Delta$ is known, or at least that there is a reasonable prior estimate of $\Delta$. Minimizing \eqref{eq-young-whittle} has the advantage that the problem becomes a form of principal component analysis (PCA), in which the solution can be computed with a single SVD. It also fits most of the
components of the full system \eqref{eq-cfa1}-\eqref{eq-cfa6}. 

It is tempting to use block relaxation here too, alternating finding the optimum
$T$ and $F$ for fixed $\Delta$, and then repeating this iteratively with the new $\Delta$ equal to
\begin{equation}
\Delta^2=n^{-1}\text{diag}\ (X-TF')'(X-TF').\label{eq-yw}
\end{equation}
This technique actually corresponds to alternating minimization of the loss function
\begin{equation}
\sigma(T,F,\Delta)=n\log\text{det}(\Delta^2)+\text{tr}\ (X-TF')\Delta^{-2}(X-TF')'.\label{eq-lawley41}
\end{equation}
But \eqref{eq-lawley41} is the negative maximum likelihood loss function of @lawley_41, and we know it is unbounded below, does not have a minimum, and converges to a solution which has $p$ of the uniquenesses in $\Delta^2$ equal to zero (@anderson_rubin_56).

A more direct way of fitting the full CFA model was discussed (independently and around the same time) in the dissertation of @socan_03 and in the conference chapter of @deleeuw_C_04a (presented at the conference in 2002). Recently this technique has become known as Matrix Decomposition Factor analysis (MDFA). Socan attributes MDFA (which they call *Direct Factor Analysis*) to a 2001 personal communication and some unpublished notes of his advisor Henk Kiers.

In MDFA the least squares loss function is
\begin{equation}
\sigma(Y,A):=\text{SSQ}(X-YA'),\label{eq-lossdef}
\end{equation}
which must be minimized over $Y'Y=I$ and $A\in\mathcal{A}$.
MDFA was not immediately accepted as an alternative factor analysis technique. It made its first journal appearance in a series of papers by Steffen Unkel and Nickolay T. Trendafilov, based largely on Unkel's dissertation (@unkel_09). Over the years they contributed a "robust"
version of MDFA and a version for a "wide" data matrux $X$ (@trendafilov_unkel_11)
There is a nice review of these contributions in @unkel_trendafilov_10. 
@trendafilov_unkel_krzanowski_13

K. Adachi contributed some useful results to what they initially called *Data-Fitting Factor Analysis*. @adachi_12, 

@adachi_trendafilov_18
@stegeman_16 Data Factor Model

@yamashita_25, @terada_24a, @terada_24b

\sectionbreak

# MDFA Algorithms

## Alternating Least Squares

The Alternating Least Squares (ALS) algorithm proposed for MDFA
by both Kiers (in @socan_03) and @deleeuw_C_04a alternates finding 
the optimum $Y$ for given $A$ and the optimum $A$ for given $Y$.

### Optimum Loadings for Given Scores

Finding the optimum $A$ for given $Y$ is  straightforward.
We complete the square, as in
\begin{multline}
\sigma_\star(Y,A)=\text{tr}\ X'X+\text{tr}\ A'A-2\text{tr}\ Y'XA=\\
\text{tr}\ X'X+\text{tr} (A-X'Y)'(A-X'Y)-\text{tr}\ Y'X'XY.
\end{multline}
Thus the optimum $A$ for given $Y$ is obtained by projecting 
$X'Y$ on the convex set of matrices $\mathcal{A}$. 

For CFA this gives 
$$
A=\begin{bmatrix}F&D\end{bmatrix}=\begin{bmatrix}X'T&\text{diag}(X'U)\end{bmatrix}.
$$ 
For confirmatory MDFA in which we require some loadings to be 
equal to each other and/or others to be equal to given constants finding $A$ is still a simple linear least squares problem. If there are inequality constraints we need some form of quadratic programming.

### Optimum Scores for given Loadings

Suppose $Z$ is $n\times p$ of rank $r$. We are looking for the 
the maximum of $\text{tr}\ Y'Z$ if $Y$ varies over the $n\times p$
columnwise orthonormal matrices.  The following theorem
was given earlier, with somewhat different terminology, in the Appendix of @deleeuw_C_04a. Remember that the *nuclear norm* or 
*trace norm* $\|Z\|_\star$ of a matrix $Z$ is the sum of its singular values.

::: {#prp-procrustus}
$$
\max_{Y'Y=I}\text{tr}\ Y'Z=\|Z\|_\star
$$
:::

:::{.proof}
The stationary equations are $Z=YM$ with $Y'Y=I$ and $M$ a symmetric matrix of Lagrange multipliers. It follows that $M^2=Z'Z$ and thus $M$ is the (unique) positive semi-definite
symmetric square root $\smash{(Z'Z)^\frac12}$
(or its negative, for a minimum). At the maximum $\text{tr}\ Y'Z=\text{tr}\ M=\|Z\|_\star$.
:::

Note that @prp-procrustus does not say that the optimizing $Y$ is unique. We get more insight from an alternative and more constructive proof (also taken from @deleeuw_C_04a, Appendix). It uses the "fat" singular value decomposition of the $n\times m$ matrix $Z$ of rank $r$, which is
$$
Z=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}\Lambda&0\\0&0\end{bmatrix}
\begin{bmatrix}L'&L_\perp'\end{bmatrix},
$$
where the matrix $\Lambda$ is of order $r$, diagonal, and positive
definite. 

::: {#thm-construct}
The $n\times m$ orthonormal $Y$ maximizes $\text{tr}\ Y'X$ if and only if
$$
Y=KL'+K_\perp^{\ }SL_\perp'
$$
for some $(n-r)\times(m-r)$ matrix $S$ with $S'S=I$.
:::

::: {.proof}
Partition the columnwise orthonormal $n\times m$ matrix 
$Y$ in the same way as $Z$.
\begin{equation}
Y=\begin{bmatrix}K&K_\perp\end{bmatrix}
\begin{bmatrix}P&Q\\R&S\end{bmatrix}
\begin{bmatrix}L'&L_\perp'\end{bmatrix},\label{eq-odecomp}
\end{equation}
where the four-block partitioned matrix in the middle of \eqref{eq-odecomp} must be orthonormal again. Now $\text{tr}\ Y'X=\text{tr}\ \Lambda P$ and the constraint on the $r\times r$ matrix $P$ is that $P'P\lesssim I$ in the Loewner sense (because we must have
$P'P+R'R=I$). It follows that the diagonal elements of $P$ must be less than or equal to one, and thus the maximum of
$\text{tr}\ Y'X$ is $\text{tr}\ \Lambda$, uniquely attained for $P=I$.
But $P=I$ implies that at the maximum $R=0$ and $Q=0$, and $S$ is any
$(n-r)\times(m-r)$ matrix with $S'S=I$. Thus the optimum $Y$ is
$$
Y=KL'+K_\perp^{\ } SL_\perp'.
$$
If $r=n$ or $r=m$ then there is no $S$ and the solution is unique.
:::

$Z=XA$ thus $Y'Z=LK'XA=L\Lambda L'A=(A'CA)^\frac12A$

Selection oF $S$, Continuous selection. Adachi.
\sectionbreak

## Majorization

### Loss Function

Define $C:=X'X$ and for any real symmetric matrix 
write $\lambda_s()$ for its eigenvalues, in decreasing order. 
The following proposition was proved in @deleeuw_C_04a, page 128,
but not really used for anything.

::: {#prp-projection}
$$
\sigma_\star(A)=\text{tr}\ X'X+\text{tr}\ A'A-2\|XA\|_\star
$$
:::

::: {.proof}
Clearly
\begin{equation}
\sigma_\star(A)=\text{tr}\ X'X+\text{tr}\ A'A-2\max_{Y'Y=I}\text{tr}\ Y'XA,\label{eq-dc}
\end{equation}
Now apply @prp-procrustus.
:::

@horn_johnson_91, theorem 3.3.14, p. 177

$$
\|XA\|_\star\leq\sum_{i=1}^m\lambda_i(X)\lambda_i(A)
$$
$$
\sigma_\star(XA)\geq\sum_{i=1}^m(\lambda_i(X)-\lambda_i(A))^2
$$
### Subgradient

$$
f(A)=\|XA\|_\star=\max_{Y'Y=I}\text{tr}\ A'X'Y
$$
$$
\|XA\|_\star\geq\|X\tilde A\|_\star+\text{tr}\ (A-\tilde A)'X'\tilde K\tilde L'=\text{tr}\ A'X'\tilde K\tilde L'
$$
$$\sigma(A)=\text{tr}\ C+\text{tr} A'A-2\|XA\|_\star\leq\text{tr}\ C+\text{tr} A'A-2\text{tr}\ A'X'\tilde K\tilde L'
$$
Define $\overline A:=X'\tilde K\tilde L'$ then
$$
\sigma(A)\leq\text{tr}\ C+\text{SSQ}(A-\overline A)-SSQ(\overline A)
$$
Now $$\tilde K\tilde L'=X\tilde A(\tilde A'X'X\tilde A)^{-\frac12}$$
and thus 
$$
\overline A=CX\tilde A(\tilde A'C\tilde A)^{-\frac12}
$$


### Gradient

$V=A'CA$ with eigenvalues $\lambda_s$ and eigenvectors
$x_s$, where $s=1,\cdots m$. V(A) is of order $p$ and
of rank $m$. $A$ is $m\times p$ with elements $a_{ij}$.
Thus $\mathcal{D}_{(ij)}$ is the partial derivative with
respect to element $a_{ij}$.

\begin{align}
\mathcal{D}_{(ij)}\lambda_s(A)&=x_s'(A)[\mathcal{D}_{(ij)}A'CA]x_s(A),\\
\mathcal{D}_{(ij)}x_s(A)&=-(A'CA-\lambda_s(A)I)^+[\mathcal{D}_{(ij)}A'CA]x_s(A)
\end{align}
In ... we use the Moore-Penrose inverse of $V(A)-\lambda_s(A)I$. 

Now for $V(A)=A'CA$ we have
$$
\mathcal{D}_{(ij)}A'CA=e_je_i'CA+A'Ce_ie_j'
$$

$$
\mathcal{D}_{(ij)}\lambda_s(A)=2x_s'(A)e_j\ e_i'CAx_s(A)
$$

Now consider $\sqrt{\lambda(V(A))}$. 
$$
\mathcal{D}_{(ij)}\sqrt{\lambda_s(V(A))}=\frac{1}{\sqrt{\lambda_s(V(A))}}
x_s'e_je_i'CAx_s
$$
Sum this over $s$ and put all the partials in a matrix.
$$
\mathcal{D}f(A)=CA(A'CA)^{-\frac12}
$$
where $(A'CA)^{-\frac12}$ is the symmetric square root of the Moore-Penrose inverse of $A'CA$ (and *not* the inverse of the symmetric square root of $A'CA$, which is singular for $p>m$)

With this formula for the gradient many general purpose optimization methods for minimizing projected loss function ... become available. The R function
... (code in the appendix) uses the BFGS method of the optim() function from the stats package
(@r_core_team_25). Our example is the correlation matrix of order 9 from 
@emmett_49, also used in @lawley_maxwell_71, pp. 42-46. We use three
common factors, and compare our results with those obtained using
multinormal maximum likelihood (@lawley_maxwell_71, table 4.2).

```{r}
matrixPrint(emmett, digits = 3, width = 5, flag = "")
```
```{r}
aeig <- eigen(emmett)
avec <- aeig$vectors[, 1:3]
aval <- diag(sqrt(aeig$values[1:3]))
acom <- avec %*% aval
a <- cbind(acom, diag(sqrt(1 - rowSums(acom^2))))
t <- cbind(matrix(1, 9, 3), diag(9))
a[1, 2] <- a[1, 3] <- a[2, 3] <- 0
t[1, 2] <- t[1, 3] <- t[2, 3] <- 0
h <- optProj(emmett, a, t)
print(h)
```
```{r}
a <- cbind(acom, diag(sqrt(1 - rowSums(acom^2))))
t <- cbind(matrix(1, 9, 3), diag(9))
h <- optProj(emmett, a, t)
print(h)
```

### Hessian

Now for the Hessian.
\begin{multline}
\mathcal{D}_{(ij)(kl)}\sqrt{\lambda_s(A'CA)}=-\frac14\frac{1}{\lambda_s^\frac32(A)}
x_s'[e_je_i'CA+A'Ce_ie_j']x_s\ x_s'[e_le_k'CA+A'Ce_ke_l']x_s+\\
\frac12\frac{1}{\sqrt{\lambda_s(A'CA)}}\{x_s'[c_{ik}(e_je_l'+e_le_j')]x_s-2x_s'[e_je_i'CA+A'Ce_ie_j'](A-\lambda_sI)^+[e_le_k'CA+A'Ce_ke_l']x_s\}
\end{multline}

$$
\mathcal{D}_{(ij)(kl)}A'CA=c_{ik}(e_je_l'+e_le_j')
$$

$CAx_s=y$
$$
4x_{js}e_i'CAx_s\ x_{ls}e_k'CAx_s
$$
$$
x_s'[c_{ik}(e_je_l'+e_le_j')]x_s=2c_{ik}x_{js}x_{ls}.
$$
$$
x_s'[e_je_i'CA+A'Ce_ie_j'](A-\lambda_sI)^+[e_le_k'CA+A'Ce_ke_l']x_s=
$$


\sectionbreak

# Statistics

Now define $C:=X'X$ and for any square symmetric T use the notation
$\lambda_s(T)$ for the ordered eigenvalues of $T$. Define
$$
\sigma(A,C):=\text{tr}\ C+\text{tr}\ A'A-2\sum_{s=1}^m\sqrt{\lambda_s(A'CA)}
$$
Differentiate this with repect to $A$, assuming the eigenvalues
are all different.
$$
\mathcal{D}_1\sigma(A,C)=2\{A-CA(A'CA)^{-\frac12}\}
$$
Note that $\mathcal{D}_1\sigma(A,C)=0$ if $C=AA'$.

1. The minimizer $A(C)$ is a continuous function of $C$.
2. The minimizer $A(C)$ is a differentiable function of $C$.


1. $\underline{C}_n$ converges weakly to $\Gamma=AA'$.
2. $\smash{n^{-\frac12}(\underline{C}_n-\Gamma)}$ is asymptotically normal.


\sectionbreak


# Code


# References
